![Billumy-logo](https://portifolio.dinho.dev/media/projetos/Billumy_site.png)

# ğŸ§  Billumy â€“ Assistente de IA com Ollama

### Modelos customizados, embeddings e arquitetura segura com Nginx + NMS

A **Billumy** Ã© uma assistente de IA especializada em anÃ¡lise de dados, construÃ­da sobre modelos **Qwen 2.5** personalizados e executada dentro do ambiente **Ollama**.  
O projeto utiliza um pipeline seguro de validaÃ§Ã£o de acesso com **Nginx + Nubo Management System (NMS)** para garantir isolamento, autenticaÃ§Ã£o e controle granular sobre cada rota de modelo.

---

## ğŸš€ VisÃ£o Geral da Arquitetura

A arquitetura deste projeto combina:

### **1. Ollama**

Servindo os modelos customizados:

- `billumy-qwen14b` â†’ rÃ¡pido e econÃ´mico (14B parÃ¢metros)
- `billumy-qwen32b` â†’ raciocÃ­nio mais robusto (32B parÃ¢metros)
- `billumy-maestro` â†’ classificador de complexidade de perguntas (1.5B parÃ¢metros)
- `mxbai-embed-large` â†’ geraÃ§Ã£o de embeddings

### **2. Nginx (Servidor)**

Configurado como **site em `sites-available`** para:

- interceptar todas as requisiÃ§Ãµes HTTP/S
- validar o **Token de OperaÃ§Ã£o (TOC)** via NMS
- rotear o trÃ¡fego para o servidor Ollama
- garantir logs centralizados, isolamento e regras de acesso corporativas

### **3. Nubo Management System (NMS)**

ResponsÃ¡vel por:

- autenticaÃ§Ã£o e autorizaÃ§Ã£o do TOC
- validaÃ§Ã£o de permissÃµes para rotas e modelos especÃ­ficos
- registro de auditoria
- controle granular de uso de modelos de linguagem corporativos

### **4. Billumy (Prompt Engineering)**

Os modelos carregam um sistema prompt personalizado:

> â€œVocÃª Ã© a Billumy, uma assistente de IA especializada em anÃ¡lise de dados. Todas as respostas devem ser em portuguÃªs, com explicaÃ§Ãµes claras, educadas e profissionais. Este projeto Ã© parte do TCC de Anderson Freitas, sob orientaÃ§Ã£o do Prof. Dr. Ary Henrique Morais Oliveira e Prof. Dr. Eduardo Ribeiro.â€

---

## ğŸ›¡ï¸ SeguranÃ§a e Controle de Acesso

A combinaÃ§Ã£o **Nginx + TOC + NMS** fornece:

- ğŸ” **AutenticaÃ§Ã£o obrigatÃ³ria**
- ğŸ§© **ValidaÃ§Ã£o de permissÃ£o por modelo (14B, 32B, embeddings)**
- ğŸ” **Auditoria centralizada**
- ğŸ§± **Isolamento entre instÃ¢ncias**
- âš™ï¸ **Consulta dinÃ¢mica de permissÃµes antes de rotear ao Ollama**

Isso permite que cada chamada ao modelo seja controlada, rastreÃ¡vel e alinhada com polÃ­ticas corporativas.

> ObservaÃ§Ã£o: A configuraÃ§Ã£o do Nginx fica no servidor, em `/etc/nginx/sites-available/billumy` (ou similar), e nÃ£o dentro do container.

---

## ğŸ—ï¸ Estrutura do Projeto

```
/
â”œâ”€ Modelfile-qwen14b      # Modelo Qwen 14B customizado
â”œâ”€ Modelfile-qwen32b      # Modelo Qwen 32B customizado
â”œâ”€ Modelfile-qwenMaestro  # Modelo classificador de complexidade
â”œâ”€ entrypoint.sh          # Script de inicializaÃ§Ã£o automÃ¡tica
â”œâ”€ Dockerfile             # Imagem Docker do Ollama
â”œâ”€ docker-compose.yml     # OrquestraÃ§Ã£o do ambiente
â””â”€ README.md              # DocumentaÃ§Ã£o do projeto
```

---

## ğŸ”§ Como funciona a inicializaÃ§Ã£o

O script `entrypoint.sh`:

1. Sobe o servidor Ollama
2. Aguarda ele ficar disponÃ­vel
3. Cria automaticamente:
   - `billumy-qwen14b` (a partir do Modelfile-qwen14b)
   - `billumy-qwen32b` (a partir do Modelfile-qwen32b)
   - `billumy-maestro` (a partir do Modelfile-qwenMaestro)
4. Puxa o modelo de embeddings na primeira execuÃ§Ã£o
5. Acessos HTTP passam pelo **Nginx configurado no servidor**, que valida o TOC antes de rotear ao Ollama

> **Nota importante**: Os Modelfiles usam o comando `SYSTEM` (nÃ£o `PARAMETER system`) para definir o prompt do sistema, conforme especificaÃ§Ã£o do Ollama.

---

## ğŸ§ª Exemplos de Uso

### Chat com o modelo 14B

```bash
curl https://billumy.a6n.tech/api/chat -H "Authorization: Bearer <TOC>" -d '{
  "model": "billumy-qwen14b",
  "messages": [{"role": "user", "content": "OlÃ¡, Billumy!"}]
}'
```

### ClassificaÃ§Ã£o de complexidade com Maestro

```bash
curl https://billumy.a6n.tech/api/chat -H "Authorization: Bearer <TOC>" -d '{
  "model": "billumy-maestro",
  "messages": [{"role": "user", "content": "Qual Ã© a capital do Brasil?"}]
}'
```

### GeraÃ§Ã£o de Embeddings

```bash
curl https://billumy.a6n.tech/api/embed -H "Authorization: Bearer <TOC>" -d '{
  "model": "mxbai-embed-large",
  "input": "Texto para embutir"
}'
```

(Nginx valida o TOC antes da requisiÃ§Ã£o chegar ao Ollama)

---

## ğŸ“¦ Modelos Utilizados

### ğŸ§© billumy-qwen14b

Baseado no **Qwen2.5 14B**, balanceado entre velocidade e qualidade. Ideal para consultas rÃ¡pidas e anÃ¡lise de dados.

### ğŸ§© billumy-qwen32b

Baseado no **Qwen2.5 32B**, ideal para raciocÃ­nio complexo e respostas longas e detalhadas.

### ğŸ¯ billumy-maestro

Baseado no **Qwen2.5 1.5B**, classificador especializado que identifica a complexidade de perguntas (simples, mÃ©dia ou profunda) para roteamento inteligente.

### ğŸ” mxbai-embed-large

Modelo de embeddings de alta performance para uso em pipelines RAG e buscas semÃ¢nticas.

---

## ğŸ› ï¸ CustomizaÃ§Ã£o

- **Temperatura**
- **Contexto mÃ¡ximo**
- **Prompt do sistema Billumy**
- **Regras de acesso no NMS**
- **InterceptaÃ§Ã£o e roteamento via Nginx no servidor**

---

## ğŸ“š Tecnologias Empregadas

- **Ollama** â€“ Servidor local de LLMs
- **Qwen2.5** â€“ Base dos modelos de linguagem
- **mxbai-embed-large** â€“ Embeddings otimizados
- **Nginx (servidor)** â€“ Proxy reverso + camada de seguranÃ§a
- **NMS (Nubo Management System)** â€“ AutorizaÃ§Ã£o corporativa
- **Docker** â€“ Empacotamento do ambiente
- **Shell Script** â€“ AutomaÃ§Ã£o do bootstrap

---

## ğŸ“„ LicenÃ§a

MIT. Livre para usar, modificar e contribuir.
